<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><title>Improving Text Rendering In Image Generation Models</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)">
    <meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)">
    <!-- html -->
    <meta name="src" content="main.tex">
    <link rel="stylesheet" type="text/css" href="main.css">
</head>
<body
>
<div class="maketitle">


    <h2 class="titleHead">Improving Text Rendering In Image Generation Models</h2>
    <div class="author"><span
            class="ptmr7t-x-x-120">Shengfan Cao</span>
        <br/><span
                class="pcrr7t-x-x-90">shengfan</span><span
                class="pcrr7t-x-x-90">_cao@berkeley.edu </span><br class="and"/><span
                class="ptmr7t-x-x-120">Xiao Huang</span>
        <br/> <span
                class="pcrr7t-x-x-90">xiao.huang@berkeley.edu </span></div>
    <br/>
</div>
<div
        class="abstract"
>
    <div
            class="centerline">                  <span
            class="ptmb7t-x-x-120">Abstract</span></div>
    <p class="indent">
    <span
            class="ptmri7t-">Rendering accurate and coherent text in image generation</span>
        <span
                class="ptmri7t-">models remains a significant challenge, particularly in scenes</span>
        <span
                class="ptmri7t-">with intricate text details. Existing methods often struggle with</span>
        <span
                class="ptmri7t-">rendering fine-grained text, especially in dense layouts or when</span>
        <span
                class="ptmri7t-">text is embedded in the background.</span>
        <!--l. 5-->
    <p class="indent">  <span
            class="ptmri7t-">To address these challenges, we propose a novel pipeline that</span>
        <span
                class="ptmri7t-">autonomously identifies and corrects misrendered text in</span>
        <span
                class="ptmri7t-">images generated by any text-to-image model. Our approach</span>
        <span
                class="ptmri7t-">leverages optical character recognition (OCR) to detect</span>
        <span
                class="ptmri7t-">problematic text regions, an LLM to refine and suggest</span>
        <span
                class="ptmri7t-">contextually appropriate corrections, and an inpainting process</span>
        <span
                class="ptmri7t-">using a modified diffusion model to seamlessly integrate</span>
        <span
                class="ptmri7t-">corrected text into the image. Experiments demonstrate that</span>
        <span
                class="ptmri7t-">our pipeline significantly improves text quality, achieving</span>
        <span
                class="ptmri7t-">higher OCR confidence scores and generating visually</span>
        <span
                class="ptmri7t-">coherent text in diverse scenarios. Our method offers a robust</span>
        <span
                class="ptmri7t-">framework for enhancing text rendering in image generation</span>
        <span
                class="ptmri7t-">models and sets the stage for future advancements in this</span>
        <span
                class="ptmri7t-">domain.</span>
</div>
<h3 class="sectionHead"><span class="titlemark">1    </span> <a
        id="x1-10001"></a>.&#x00A0;Introduction</h3>
<!--l. 4--><p class="noindent">Rendering of accurate and coherent text in image generation
    models remains an open challenge. While it is possible to
    infer the structure of an image and propose reasonable
    text given the context, the model tends to struggle with
    rendering the characters. Common errors include incorrect
    semantics, homophones, and issues such as added, repeated,
    merged, or dropped glyphs, as well as misshapen characters
    [<a
            href="#Xliu_character-aware_2023">5</a>].
    <!--l. 11-->
<p class="indent"> State-of-the-art approaches have primarily focused on
    integrating larger language models and character-aware text
    encoders. While these methods improve text rendering
    in prominent areas of an image, they often fall short in finer details, such as background text, or in cases where
    text is densely clustered. Additionally, their performance
    significantly declines when explicit keywords are absent from
    the prompt, highlighting the need for improved strategies in text
    rendering.
    <!--l. 15-->
<p class="indent"> In this project, we introduce a novel pipeline to address these
    challenges by rectifying misrendered text in images generated
    by any text-to-image model. Our pipeline offers two key
    contributions:
<dl class="enumerate-enumitem">
    <dt class="enumerate-enumitem">
        1.
    </dt>
    <dd
            class="enumerate-enumitem">Autonomous identification and correction of
        misrendered text using inpainting techniques.
    </dd>
    <dt class="enumerate-enumitem">
        2.
    </dt>
    <dd
            class="enumerate-enumitem">Automatic inference of contextually appropriate text
        without requiring explicit prompts.
    </dd>
</dl>
<!--l. 21--><p class="indent"> This approach aims to enhance the accuracy and coherence
    of text rendering across diverse scenarios.


    <!--l. 24-->
<p class="indent">
<hr class="float">
<div class="float"
>


    <img
            src="imgs/gibberish_text.jpg" alt="PIC" width="100%"> <a
        id="x1-1003r1"></a>
    <br/>
    <div class="caption"
    ><span class="id">
<span
        class="ptmr7t-x-x-90">Figure</span><span
            class="ptmr7t-x-x-90">&#x00A0;1. </span></span><span
            class="content"><span
            class="ptmr7t-x-x-90">Gibberish text rendered by SOTA models without explicit keywords in the prompt.</span>                               </span>
    </div><!--tex4ht:label?: x1-1003r1 -->


</div>
<hr class="endfloat"/>
<h3 class="sectionHead"><span class="titlemark">2    </span> <a
        id="x1-20002"></a>.&#x00A0;Related Work</h3>
<!--l. 2--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">2.1    </span> <a
        id="x1-30002.1"></a>.&#x00A0;Visual Text Rendering</h4>
<!--l. 4--><p class="noindent">Liu et. al [<a
        href="#Xliu_character-aware_2023">5</a>] suggested that the problem primarily stems from
    the absence of a character-aware text encoder during training.
    Character-aware models enhance text rendering more effectively
    than character-blind models. The DeepFloyd IF model
    incorporated an LLM as a text encoder, leveraging the
    text-image cross-attention to enhance the coherence of
    generated texts [<a
            href="#Xsaharia_photorealistic_2022">8</a>]. However, this class of solutions is criticized
    for lacking controllability and flexibility[<a
            href="#Xchen_textdiffuser-2_2023">2</a>].
    <!--l. 9-->
<p class="indent"> Another class of solutions is to divide the task into multiple
    steps. The TextDiffuser [<a
            href="#Xchen_textdiffuser_2023">3</a>] consists of a two-stage pipeline,
    where the first stage utilizes a character-level layout, and the
    second stage employs a diffusion model to generate the entire
    image. A character-aware loss function is applied to maintain
    the coherence of the generated text. The subsequent work,
    TextDiffuser-2 [<a
            href="#Xchen_textdiffuser-2_2023">2</a>], further leverages an LLM to generate the
    style and layout of possible text prompts, and utilizes the
    language model within the diffusion model to encode the
    position and content of the text at the line level. Similarly, the
    GlyphDraw learning framework [<a
            href="#Xma_glyphdraw_2023">6</a>] addressed the issue by
    incorporating glyph and position information and was
    successful in rendering complex Chinese characters in
    images.
    <!--l. 15-->
<p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">2.2    </span> <a
        id="x1-40002.2"></a>.&#x00A0;Optical Character Recognition (OCR)</h4>
<!--l. 16--><p class="noindent">OCR leverages machine learning and image processing
    techniques to identify and extract textual information from
    digital images or scanned documents. The process involves
    several stages, including image pre-processing (such as noise
    reduction and binarization), text localization (detecting areas
    that contain text), and character recognition (mapping image
    features to a character set). Modern OCR systems often use
    deep learning-based models, such as Convolutional Neural
    Networks (CNNs) or Transformer architectures, to enhance
    accuracy and handle variations in font, size, and distortion. In
    this project, OCR is integrated into the pipeline to automatically
    detect and extract text from images, facilitating further
    processing and analysis.
    <!--l. 2-->
<p class="noindent">
<h3 class="sectionHead"><span class="titlemark">3    </span> <a
        id="x1-50003"></a>.&#x00A0;Proposed Method</h3>
<!--l. 3--><p class="noindent">Our proposed pipeline is shown in Fig&#x00A0;<a
        href="#x1-5001r2">2<!--tex4ht:ref: fig:pipeline --></a>. The pipeline serves as
    a post-inference filter to rectify incorrectly rendered texts in
    images. The pipeline primarily consists of three parts: OCR models for detection, LLM for judgement and suggestion,
    and
    finally Text Diffuser model for repainting.


    <!--l. 11-->
<p class="indent">
<hr class="float">
<div class="float">
    <img src="imgs/architecture.png" alt="PIC" width="100%"> <a id="x1-5001r2"></a>
    <br/>
    <div class="caption">
    <span class="id">
    <span class="ptmr7t-x-x-90">Figure</span>
    <span class="ptmr7t-x-x-90">&#x00A0;2. </span>
    </span>
        <span class="content">
            <span class="ptmr7t-x-x-90">Proposed architecture.</span>
        </span>
    </div><!--tex4ht:label?: x1-5001r2 -->
</div>
<hr class="endfloat"/>


<!--l. 20--><p class="indent">
<hr class="float">
<div class="float"
>


    <div class="subfigure">
        <!--l. 22--><p class="noindent"><!--l. 24-->
        <p class="noindent"><img
                src="imgs/pipline0.png" alt="PIC"
                width="50%"> <a
                id="x1-5002r1"></a>
        <div class="caption"><span class="id"><span
                class="ptmr7t-x-x-80">(a) </span></span><span
                class="content"><span
                class="ptmr7t-x-x-80">Original Generated Image</span>                    </span></div>
    </div>
    <div class="subfigure">
        <!--l. 28--><p class="noindent"><!--l. 30-->
        <p class="noindent"><img
                src="imgs/pipeline1.jpg" alt="PIC"
                width="50%"> <a
                id="x1-5003r2"></a>
        <div class="caption"><span class="id"><span
                class="ptmr7t-x-x-80">(b) </span></span><span
                class="content"><span
                class="ptmr7t-x-x-80">Detection through OCR</span>                     </span></div>
    </div>
    <div class="subfigure">
        <!--l. 34--><p class="noindent"><!--l. 36-->
        <p class="noindent"><img
                src="imgs/pipeline2.png" alt="PIC"
                width="50%"> <a
                id="x1-5004r3"></a>
        <div class="caption"><span class="id"><span
                class="ptmr7t-x-x-80">(c) </span></span><span
                class="content"><span
                class="ptmr7t-x-x-80">LLM Suggestion</span>                         </span></div>
    </div>
    <div class="subfigure">
        <!--l. 40--><p class="noindent"><!--l. 43-->
        <p class="noindent"><img
                src="imgs/pipline3.png" alt="PIC"
                width="50%"> <a
                id="x1-5005r4"></a>
        <div class="caption"><span class="id"><span
                class="ptmr7t-x-x-80">(d) </span></span><span
                class="content"><span
                class="ptmr7t-x-x-80">Final Output</span>                            </span></div>
    </div>
    <a
            id="x1-5006r3"></a>
    <div class="caption"><span class="id"><span
            class="ptmr7t-x-x-90">Figure</span><span
            class="ptmr7t-x-x-90">&#x00A0;3. </span></span><span
            class="content"><span
            class="ptmr7t-x-x-90">Intermediate results in various stages. In sub-figure (b), the green boxes are the detected text-like textures by OCR. The identified</span>
<span
        class="ptmr7t-x-x-90">text and the confidence score are displayed above each box. In sub-figure (c), the red boxes are the mask for inpainting after merging, and</span>
<span
        class="ptmr7t-x-x-90">the text is the suggested text from LLM.</span>
</span></div>


</div>
<hr class="endfloat"/>
<h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a
        id="x1-60003.1"></a>.&#x00A0;Text Identification with OCR</h4>
<!--l. 51--><p class="noindent">Specifically, we used the default setting of easyOCR [<a
        href="#Xjaidedai2024easyocr">4</a>], using
    CRAFT Algorithm for detection execution. The recognition
    model uses a Convolutional Recurrent Neural Network (CRNN)
    [<a
            href="#Xshi2015endtoendtrainableneuralnetwork">9</a>], composed of 3 main components: feature extraction
    with
    Resnet, sequence labeling with LSTM, and decoding with
    CTC.
    <!--l. 53-->
<p class="indent"> We found that OCR can reliably identify regions with
    text-like textures in images, even when the text is not perfectly
    rendered. Each detected region is linked to the recognized
    text and its associated confidence level. To prevent overly
    clustered regions during inpainting, we discard detections with
    small areas and merge overlapping ones. Additionally, we
    slightly expand the identified regions to ensure that the
    original text is fully masked. Once the OCR model processes
    the input image, it generates outputs that include both the
    recognized text and the spatial locations of the detected
    regions.
    <!--l. 55-->
<p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a
        id="x1-70003.2"></a>.&#x00A0;Language Model as Judge</h4>
<!--l. 56--><p class="noindent">The outputs from OCR are passed into a Large Language Model
    (LLM) to refine, validate, and contextualize the extracted
    text. The LLM functions as an intelligent agent, analyzing
    the recognized text for potential errors, ambiguities, or
    inconsistencies, such as out-of-context information. By
    leveraging its deep contextual understanding, the LLM
    evaluates the OCR results and provides well-informed
    suggestions for corrections or enhancements.
    <!--l. 60-->
<p class="indent"> In this project, we utilize the GPT-4o-mini model [<a
        href="#Xopenai2024">7</a>] via
    OpenAI&#8217;s API. The OCR outputs are structured and enriched
    with textual data alongside a concise description of the source
    image&#8217;s intended content, which was used to generate the
    image. Carefully designed prompts guide the LLM to emulate a
    human review process, enabling accurate and context-sensitive
    evaluations. The exact system prompt can be found in the
    Appendix.
    <!--l. 64-->
<p class="indent"> Through our experiments, we found that spatial location
    metadata contributed minimally to performance improvement,
    as the image itself is not part of the LLM&#8217;s input. Instead,
    relying on the extracted textual data, supplemented with a
    few in-context learning examples, consistently yielded
    good results in validating and enhancing the OCR outputs
    [<a
            href="#Xbrown2020languagemodelsfewshotlearners">1</a>].
    <!--l. 67-->
<p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a
        id="x1-80003.3"></a>.&#x00A0;Inpainting with TextDiffuser-2</h4>
<!--l. 68--><p class="noindent">With the masks from OCR and the suggested text from LLM,
    we follow the TextDiffuser-2 pipeline, breaking down the text into individual characters and attaching the bounding
    box
    location in the prompt. The text encoder is specially designed
    to assign a distinct encoding for each possible character
    and pixel coordinate so that the diffusion model is trained
    to generate the correct glyphs for the characters in the
    intended layout. In addition, the model is trained to choose
    appropriate styles for the glyphs to render given the context of
    the image, ensuring the rendered text blends well into the
    image.
    <!--l. 72-->
<p class="indent"> In the inference phase, we implement classifier-free
    guidance (CFG), which significantly improves the quality of
    generated images. Also, we use the inpainting technique
    to apply our mask to ensure the non-text regions remain
    unaltered.
<h3 class="sectionHead"><span class="titlemark">4    </span> <a
        id="x1-90004"></a>.&#x00A0;Experiments</h3>
<!--l. 2--><p class="noindent">We used Meta Emu as the original text-to-image generation
    model and applied our pipeline to fix the texts. We designed our
    text prompt to incur images that entail text in the background.
    The results are shown in Fig.&#x00A0;<a
            href="#x1-9005r4">4<!--tex4ht:ref: fig:2x3_grid --></a>.
    <!--l. 6-->
<p class="indent"> To quantify the quality of the text generated by our pipeline,
    we used the OCR model to analyze the original and filtered
    images for confidence scores. We select the median of the
    confidence score as our criteria. The results are shown in
    Table&#x00A0;<a
            href="#x1-9001r1">1<!--tex4ht:ref: tab:results --></a>.


    <!--l. 10-->
<p class="indent">
<hr class="float">
<div class="float"
>


    <div class="tabular">
        <table id="TBL-2" class="tabular"

        >
            <colgroup id="TBL-2-1g">
                <col
                        id="TBL-2-1">
            </colgroup>
            <colgroup id="TBL-2-2g">
                <col
                        id="TBL-2-2">
            </colgroup>
            <colgroup id="TBL-2-3g">
                <col
                        id="TBL-2-3">
            </colgroup>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-1-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"
                    class="td11"><span
                        class="ptmb7t-">Prompt                                 </span></td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-1-2"
                    class="td11"><span
                        class="ptmb7t-">Original Image</span></td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-1-3"
                    class="td11"><span
                        class="ptmb7t-">Proposed Method</span></td>
            </tr>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-2-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"
                    class="td11">Billboard in the City Center
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-2-2"
                    class="td11"> 0.0681
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-2-3"
                    class="td11">    <span
                        class="ptmb7t-">0.1082          </span></td>
            </tr>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-3-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"
                    class="td11">The exterior of a burger shop
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-3-2"
                    class="td11"> 0.0821
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-3-3"
                    class="td11">    <span
                        class="ptmb7t-">0.6202          </span></td>
            </tr>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-4-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"
                    class="td11">A Christmas Postcard
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-4-2"
                    class="td11"> 0.3294
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-4-3"
                    class="td11">    <span
                        class="ptmb7t-">0.8092          </span></td>
            </tr>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-5-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"
                    class="td11">Casino hotel in Las Vegas
                </td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-5-2"
                    class="td11">   <span
                        class="ptmb7t-">0.8782        </span></td>
                <td style="white-space:nowrap; text-align:center;" id="TBL-2-5-3"
                    class="td11"> 0.4554
                </td>
            </tr>
            <tr
                    class="hline">
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
                <td>
                    <hr>
                </td>
            </tr>
            <tr
                    style="vertical-align:baseline;" id="TBL-2-6-">
                <td style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"
                    class="td11"></td>
            </tr>
        </table>
    </div>
    <a
            id="x1-9001r1"></a>
    <br/>
    <div class="caption"
    ><span class="id">
<span
        class="ptmr7t-x-x-90">Table</span><span
            class="ptmr7t-x-x-90">&#x00A0;1. </span></span><span
            class="content"><span
            class="ptmr7t-x-x-90">Comparison of OCR confidence scores between original and processed images. Larger is better.</span>
</span></div><!--tex4ht:label?: x1-9001r1 -->


</div>
<hr class="endfloat"/>
<!--l. 26--><p class="indent"> The experiments show that our pipeline successfully replaced
    gibberish text in the original images with in-context text using
    appropriate styles.


    <!--l. 29-->
<p class="indent">
<hr class="float">
<div class="float"
>

    <div class="figure">
        <img src="imgs/results.png" alt="PIC" width="100%">
    </div>
    <div class="caption" id="#x1-9005r4"><span class="id"><span
            class="ptmr7t-x-x-90">Figure</span><span
            class="ptmr7t-x-x-90">&#x00A0;4. </span></span><span
            class="content"><span
            class="ptmr7t-x-x-90">Comparison between the original images and results through the pipeline. The images above the captions are original images</span>
    <span
            class="ptmr7t-x-x-90">with generated suggestions, and below are modified images.</span>
    </span></div>


</div>
<hr class="endfloat"/>
<h3 class="sectionHead"><span class="titlemark">5    </span> <a
        id="x1-100005"></a>.&#x00A0;Limitation and Future Work</h3>
<!--l. 2--><p class="noindent">The current approach faces limitations with images containing
    dense text, such as newspapers or emails, which often lead to
    poor results. Multiscale rendering may help address this issue,
    though challenges remain, such as distorted text if not properly
    masked and overly specific image descriptions that reduce text
    diversity in LLM judgments. These issues require further tuning
    and better prompt engineering. Additionally, customizing
    layouts through Text-Diffuser is difficult, suggesting that this
    could be a focus for future work.
    <!--l. 4-->
<p class="indent"> Before inputting the OCR results into the LLM for
    suggestions, we merge overlapping detected locations into
    a larger region to avoid iteratively inpainting the same
    area, which could degrade the output text rendering. In our
    experiments, we found that slightly enlarging the labeled region
    generally improves results. However, the optimal degree of
    enlargement remains an open question and requires further
    investigation.
    <!--l. 6-->
<p class="indent"> The current evaluation metric is still in its preliminary stages.
    While it provides an intuitive numerical benchmark, a higher
    score does not necessarily indicate better quality, as this is
    inherently subjective. The metric incorporates the OCR
    confidence score, which has not been specifically tuned for this
    task. Developing a more robust evaluation metric that better
    aligns with the task&#8217;s objectives, and using it as a loss function
    to train the entire pipeline could be a direction for future
    work.
    <!--l. 1-->
<p class="noindent">
<h3 class="likesectionHead"><a
        id="x1-11000"></a><span
        class="ptmr7t-x-x-90">References</span></h3>
<!--l. 1--><p class="noindent">
<div class="thebibliography">
    <p class="bibitem"><span class="biblabel">
<a
        id="Xbrown2020languagemodelsfewshotlearners"></a><span
            class="ptmr7t-x-x-90">[1]</span> <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Tom</span><span
            class="ptmr7t-x-x-90">&#x00A0;B.  Brown,  Benjamin  Mann,  Nick  Ryder,  Melanie</span>
        <span
                class="ptmr7t-x-x-90">Subbiah,</span>
        <span
                class="ptmr7t-x-x-90">Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav</span>
        <span
                class="ptmr7t-x-x-90">Shyam,  Girish  Sastry,  Amanda  Askell,  Sandhini  Agarwal,</span>
        <span
                class="ptmr7t-x-x-90">Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon</span>
        <span
                class="ptmr7t-x-x-90">Child,   Aditya   Ramesh,   Daniel</span><span
                class="ptmr7t-x-x-90">&#x00A0;M.   Ziegler,   Jeffrey   Wu,</span>
        <span
                class="ptmr7t-x-x-90">Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,</span>
        <span
                class="ptmr7t-x-x-90">Mateusz  Litwin,  Scott  Gray,  Benjamin  Chess,  Jack  Clark,</span>
        <span
                class="ptmr7t-x-x-90">Christopher  Berner,  Sam  McCandlish,  Alec  Radford,  Ilya</span>
        <span
                class="ptmr7t-x-x-90">Sutskever, and Dario Amodei. Language models are few-shot</span>
        <span
                class="ptmr7t-x-x-90">learners, 2020.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
<a
        id="Xchen_textdiffuser-2_2023"></a><span
            class="ptmr7t-x-x-90">[2]</span> <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng</span>
        <span
                class="ptmr7t-x-x-90">Chen, and Furu Wei. TextDiffuser-2: Unleashing the power of</span>
        <span
                class="ptmr7t-x-x-90">language models for text rendering, .</span>
    </p>
    <p class="bibitem"><span class="biblabel">
<a
        id="Xchen_textdiffuser_2023"></a><span
            class="ptmr7t-x-x-90">[3]</span> <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng</span> <span
            class="ptmr7t-x-x-90">Chen, and Furu Wei.  TextDiffuser: Diffusion models as text</span>
        <span
                class="ptmr7t-x-x-90">painters, .</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xjaidedai2024easyocr"></a><span
            class="ptmr7t-x-x-90">[4]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">JaidedAI. Easyocr, 2024. Accessed: 2024-12-18.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xliu_character-aware_2023"></a><span
            class="ptmr7t-x-x-90">[5]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Rosanne  Liu,  Dan  Garrette,  Chitwan  Saharia,  William</span>
        <span
                class="ptmr7t-x-x-90">Chan, Adam Roberts, Sharan Narang, Irina Blok, R.</span><span
                class="ptmr7t-x-x-90">&#x00A0;J. Mical,</span>
        <span
                class="ptmr7t-x-x-90">Mohammad  Norouzi,  and  Noah  Constant.   Character-aware</span>
        <span
                class="ptmr7t-x-x-90">models improve visual text rendering.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xma_glyphdraw_2023"></a><span
            class="ptmr7t-x-x-90">[6]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di</span>
        <span
                class="ptmr7t-x-x-90">Niu, Haonan Lu, and Xiaodong Lin. GlyphDraw: Seamlessly</span>
        <span
                class="ptmr7t-x-x-90">rendering text with intricate spatial structures in text-to-image</span>
        <span
                class="ptmr7t-x-x-90">generation.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xopenai2024"></a><span
            class="ptmr7t-x-x-90">[7]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">OpenAI.      Gpt-4o-111  mini:  Advancing  cost-efficient</span>
        <span
                class="ptmr7t-x-x-90">intelligence, 2024. Accessed: 2024-12-18.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xsaharia_photorealistic_2022"></a><span
            class="ptmr7t-x-x-90">[8]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,</span>
        <span
                class="ptmr7t-x-x-90">Jay                   Whang,                   Emily                   Denton,</span>
        <span
                class="ptmr7t-x-x-90">Seyed  Kamyar</span><span
                class="ptmr7t-x-x-90">&#x00A0;Seyed  Ghasemipour,  Burcu</span><span
                class="ptmr7t-x-x-90">&#x00A0;Karagol  Ayan,</span>
        <span
                class="ptmr7t-x-x-90">S.</span><span
                class="ptmr7t-x-x-90">&#x00A0;Sara   Mahdavi,   Rapha</span><span
                class="ptmr7t-x-x-90">&#x00A0;Gontijo   Lopes,   Tim   Salimans,</span>
        <span
                class="ptmr7t-x-x-90">Jonathan   Ho,   David</span><span
                class="ptmr7t-x-x-90">&#x00A0;J.   Fleet,   and   Mohammad   Norouzi.</span>
        <span
                class="ptmr7t-x-x-90">Photorealistic   text-to-image   diffusion   models   with   deep</span>
        <span
                class="ptmr7t-x-x-90">language understanding.</span>
    </p>
    <p class="bibitem"><span class="biblabel">
     <a
             id="Xshi2015endtoendtrainableneuralnetwork"></a><span
            class="ptmr7t-x-x-90">[9]</span>  <span class="bibsp"><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span><span
            class="ptmr7t-x-x-90">&#x00A0;</span></span></span><span
            class="ptmr7t-x-x-90">Baoguang   Shi,   Xiang   Bai,   and   Cong   Yao.        An</span>
        <span
                class="ptmr7t-x-x-90">end-to-end trainable neural network for image-based sequence</span>
        <span
                class="ptmr7t-x-x-90">recognition  and  its  application  to  scene  text  recognition,</span>
        <span
                class="ptmr7t-x-x-90">2015.</span>
    </p>
</div>
<!--l. 1--><p class="indent">


<h3 class="sectionHead"><span class="titlemark">6    </span> <a
        id="x1-120006"></a>.&#x00A0;Appendix</h3>
<!--l. 3--><p class="noindent">
<h4 class="subsectionHead"><span class="titlemark">6.1    </span> <a
        id="x1-130006.1"></a>.&#x00A0;System Prompt for LLM</h4>
<!--l. 4--><p class="noindent">&#8220;Your role is to make judgement of whether a text seems
    reasonable in a photo or an image. You will be given a brief
    description of the image, and a list of recognized texts.
    THE recognized texts might be mis-spelled, out of context,
    unreasonable, or any combination of those. You will output only
    a tuple of two lists and nothing else. First list contains
    only &#8221;Yes&#8221; or &#8221;No&#8221; for each text in the list of recognized
    text, determine if you think the text is reasonable or not.
    Second list contains correction for each recognized text, You
    should modify the text to match the image discription. You
    should change words to more relevant and appropriate
    alternatives of correct spelling, and your choices should be
    divserse. If something sems random and wrong, replace it with
    a word that as a text would fit image description.Make
    sure two lists match each other. Example: an image with
    description &#8221;an image of Cat Cafe&#8221; and input texts list
    [&#8217;hel10&#8217;, &#8217;dog&#8217;, &#8217;kitty&#8217;,&#8217;1qw#dw&#8217;,&#8217;QPA&#8217;]. &#8217;hel1o&#8217;
    is spelled
    wrong, and should be corrected to &#8217;hello&#8217;, &#8217;dog&#8217; is not
    relevant, and should be corrected to &#8217;cat&#8217;,&#8217;1qw#dw&#8217; seems
    random and doesn&#8217;t make sense, thus should be replaced with
    something expected that you pick, such as &#8217;cat food&#8217;, &#8217;QPA&#8217;
    seems random and out of nowhere, should be replaced with
    &#8221;Cute&#8221; or &#8221;Coffee. &#8221; Then, You should output something like
    [&#8221;No&#8221;,&#8221;No&#8221;, &#8221;Yes&#8221;,&#8221;No&#8221;,&#8221;No&#8221;], and [&#8221;hello&#8221;,&#8221;cat&#8221;,
    &#8221;kitty&#8221;,&#8221;cat
    food&#8221;,&#8221;Coffee&#8221;]&#8221;

</body>
</html>

                                               
                                                                                               


